## Chapter 2      PCIe Architecture Overview //PCIe体系结构概述

### 关于前一章

前一章节为我们提供了PCI技术发展的历史，以此来建立更好的理解PCIe的基础。它主要了回顾PCI与PCI-X1.0/2.0的基础内容，目的是为了给接下来对PCIe的概述内容提供一些内容上的前因后果，更方便对PCIe进行理解。

### 关于本章

本章节对PCI Express体系结构进行了全面的介绍，旨在将本章作为一个“执行层”概述，涵盖该体系结构在高层的所有基础知识。它介绍了PCIe协议规范中给出的分层的方法，并描述了每个层级的职责作用。介绍了各种数据包类型的同时也一起介绍了用于通信和增强传输可靠性的协议。

### 关于下一章

下一章节介绍了PCI Express环境中的配置部分。它包括用于实现Function的配置寄存器的空间，一个Function如何在总线上被发现，配置事务是如何被生成和路由到正确的位置，PCI兼容空间与PCIe扩展空间之间的差异，以及软件是如何区分Endpoint（EP，端点）和Bridge。

### 2.1 PCI Express简介

PCI Express的出现代表了其前身并行总线的重大转变。作为一种串行总线，它与早期的串行设计（例如InfiniBand或者Fibre Channel）有许多的共同点，但是它仍然保持了在软件层面对PCI的向后兼容。

正如许多高速串行传输方法一样，PCIe使用双向连接方式，可以在同一时间同时进行信息的收发操作（即区分开了TX、RX，而不是像PCI一样在同一根线上作为inout）。这种模型被称为双单工连接（Dual-simplex connection），因为每个接口都有一个单工发送路径和一个单工接收路径，图 2‑1展示了这种模型。因为数据流可以同时进行双向传输，因此在技术层面上来说两个设备间的通信其实是全双工的，但是PCIe协议规范依然使用双单工这个术语，这是因为这种称呼对实际通信信道也进行了一点描述，即通信信道由两个方向的各自的单工信道组成。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image042.jpg)

图 2‑1双单工链路

用于描述设备之间信号传输路径的术语为“链路（Link）”，它由一个或以上的接收发送对组成。这样的一对接收和发送被称为一个“通道（Lane）”，协议规范允许一条链路内有1、2、4、8、12、16或32个Lane。链路内Lane的数量称为链路宽度，通常用x1、x2、x4、x8、x16以及x32来进行表示。用于权衡在实际设计中使用多少Lane的思路其实很简单：使用更多的Lane可以增加带宽，但是也会增加成本、增加空间占用以及增加功耗。更多关于这方面的信息，可以阅读“Links and Lanes（链路与通道）”这一节。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image044.jpg)

图 2‑2一个Lane（通道）

#### 2.1.1     软件的向后兼容性（Software Backward Compatibility）

PCIe设计目标中极为重要的一点就是要保持对PCI软件的向后兼容性。如果一个设计使用的是现有的系统，而且这个设计已经能够在这个现有系统中正常工作，那么想要促使它转向使用另一种系统则需要满足两件事：第一，新技术要有足够吸引人的性能提升，第二，尽可能减小从来技术迁移到新技术时的成本、风险以及工作量。对于第二点来说，在计算机中通常的做法是让那些为旧模型所编写的软件在新模型中依然可用。为了在PCIe中做到这一点，PCI中所用到的所有的地址空间要么不做更改便直接照搬，要么仅仅进行了了简单的扩展。Memory、IO和配置空间对于软件来说依然是可见的，而且连写入方式都和以前一样。因此就算是数年前为PCI而写的软件（BIOS代码、设备驱动等等）将依然可以在现在的PCIe设备上使用。配置空间已经被大大的扩展，添加了许多新的寄存器来支持新的功能，但是老的寄存器依然存在且可以按照常规方式来访问他们（这部分的详细内容见“Software Compatibility Characteristic软件兼容性特性”）。

#### 2.1.2     串行传输（Serial Transport）

##### 2.1.2.1   对传输速率的需求（The Need for Speed）

很明显，串行传输模型必须要比并行的设计跑的快很多才能达到相同的带宽，这是因为串行传输一次只发送1bit数据。然而，事实证明这并不困难，过去的PCIe在2.5GT/s和5.0GT/s都能稳定的工作。之所以PCIe可以达到这样的速率，甚至达到更高的8GT/s，是因为串行传输模型克服了并行模型的不足。

l **克服问题**

通过回顾并行总线的内容我们知道，它的性能被一些问题所限制，如图 2‑3展示了其中的三个问题。首先回想一下，并行总线使用公共时钟（common clock）；信号在一个时钟沿被输出，然后在下一个时钟沿被接收方接收。这个模型的第一个问题来自于信号从发送端传输到接收端所花费的时间，称为渡越时间。渡越时间必须小于一个时钟周期，否则将会出问题，这使得难以通过继续减小时钟周期来提升速度。因为若需要继续减小时钟周期，为了让信号渡越时间依然小于时钟周期，需要布线更短并减少负载的设备数量，但是最终这样的做法都会到达极限，在实际情况下变得无法达到。第二个造成并行模型性能受限的因素是使用公共时钟时，时钟到达发送方和接收方的时刻不一致，这称为时钟歪斜。PCB布线设计人员尽力去减小时钟歪斜的值，因为时钟歪斜将会降低信号传输时间预算，但是这种歪斜永远无法彻底消除。第三个因素是信号歪斜，它指的是多bit位宽数据的各个bit到达接收端的时刻存在差异。显然，这样的多bit位宽数据在所有的bit都到达且稳定之前都不能被接收方采样，这使得我们必须去等待最慢的那一bit。

 

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image046.jpg)

图 2‑3 并行总线的局限

PCIe这样的串行传输方法是如何处理这些问题的呢？首先，信号渡越时间将不再是一个问题，因为用于指示接收端锁存数据的时钟现在已经被内置入数据流中，不在需要额外传输线来传输参考时钟。因此，无论再小的时钟周期或是再长的信号传输时间都不会产生以前的问题了，这是因为内置在数据流中的时钟必然能与数据一起到达接收方。同样地，时钟歪斜的问题将不再存在，这还是因为时钟被内置入数据流中，接收方通过恢复出数据流中的时钟来进行数据采样，自然不存在时钟歪斜的问题。最后，信号歪斜的问题在一个Lane（通道）内被消除了，因为一个Lane一次只传输1bit数据。在多通道设计中，虽然信号歪斜的问题再次出现了，但是接收端将自动对其进行纠正，它可以修复大量歪斜。虽然串行设计克服了并行模型的许多问题，但是串行设计自身也有一系列的复杂问题。不过，我们稍后将看到，对这些问题的解决方法是易于控制的，并可以让我们做到高速的、可靠的通信。

l **带宽**

PCIe支持的高速且含有多Lane（通道）的链路带来了傲人的带宽数值，如表 2‑1所示。这些数字都来源于比特率与总线特性。其中一种特性与其他许多串行传输方法类似，那就是PCIe的前两代版本中使用了被称为8b/10b的编码过程，这种编码过程会根据8bit的输入而生成10bit的输出。尽管这样做会引起一些开销，但是有几个很好的理由支持我们这样做，我们将会在后面讲到。对于现在来说，只需要知道对于8b/10b编码来说，要发送1Byte的数据实际需要传送10bit。第一代PCIe（称为Gen1或者PCIe协议规范版本1.x）中，比特率为2.5GT/s，将它除以10即可得知一个Lane（通道）的速率将可以达到0.25GB/s。因为链路可以在同一时刻进行发送和接收，因此聚合带宽可以达到这个数值的两倍，即每个Lane（通道）达到0.5GB/s。第二代PCIe（称为Gen2或者PCIe 2.x）中将总线频率翻倍，这也使得它的带宽相较于Gen1翻倍。第三代PCIe（称为Gen3或者PCIe 3.0）再次让带宽翻倍，但是这次协议的制定者们并没有选择将频率翻倍。相反，出于一些原因（我们稍后将会进行讨论），他们仅将频率提升到8GT/s（Gen2中是5GT/s，未翻倍），并不再采用8b/10b的编码方式，而是采用了另一种编码机制，即128b/130b编码（关于这个内容的更多信息，可以阅读“Physical Layer-Logical（Gen3）”章节）。表 2‑1列出了当前几代PCIe的各种Lane（通道）数量情况下的带宽，展示出了链路对应情况下的峰值吞吐量。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image048.jpg)

表 2‑1 PCIe Gen1,Gen2,Gen3的各种链路宽度下的带宽汇总

##### 2.1.2.2   PCIe带宽计算方法

要计算上述表格中的PCIe带宽大小，可以参照如下的计算方法。

n Gen1 PCIe带宽=（2.5Gb/s x 2 directions）/10bits per symbol=0.5GB/s

n Gen2 PCIe带宽=（5.0Gb/s x 2 directions）/10bits per symbol=1.0GB/s

需要注意，上述计算中，我们是除以10bit而不是8bit（1Byte），这是因为Gen1和Gen2的协议中要求将Byte进行8b/10b编码后封装为10bit，然后才进行数据包的传输，因此原数据中的1Byte在实际传输时其实是需要传输10bit。

n Gen3 PCIe带宽=（2.0Gb/s x 2 directions）/8bits per byte=2.0GB/s

注意到在Gen3速率的计算中，我们除以的是8bit而不再是10bit了，这是因为Gen3中并不再使用8b/10b编码方式，而是128b/130b编码方式。这种编码方式每128bit引入2bit开销，这个开销非常小以至于我们暂且可以将它在我们的计算中忽略。

上述三种方法计算出来的带宽只需要再乘以链路宽度即可得到整个多Lane（通道）链路的链路带宽。

##### 2.1.2.3   PCIe的差分信号

每个通道都使用差分信号进行传输，差分信号是指每次传输一个信号时同时发送它的正信号和负信号（D+和D-，这两种信号振幅相同相位相反），如图 2‑4所示。当然，这样会将引脚增加一倍，但是相对于单端信号而言，差分信号在高速传输上的两个明显的优点足以抵消其引脚数方面的不足：它提高了噪声容限，并降低了信号电压。

差分信号的接收端将会接收这一对相位相反的信号，用正信号的电压减去其反相信号的电压，得到它们的差值，以此来判定这1bit的逻辑电平值。差分传输设计内置了抗噪声干扰的设计，因为它要求成对的差分信号必须位于每个设备的相邻的引脚上，它们的走点也必须彼此非常靠近，以保持合适的传输线阻抗。因此，任何因素在影响差分对中的一个信号的时候，都会同等程度且同样方向地影响到另一个信号。但是接收端所在意的是它们的差值，而这些噪声干扰并不会改变这个差值，所以带来的结果就是大多数情况下噪声对信号的影响并不会引起接收端对bit的正确判别。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image050.jpg)

图 2‑4 差分信号示意图

##### 2.1.2.4   不再使用公共时钟

在先前的内容中有提到，PCIe链路不再像PCI一样使用公共时钟（common clock），它使用了一个源同步模型（source-synchronous）,这意味着需要由发送端给接收端提供一个时钟来用于对输入数据进行锁存采样。对于PCIe链路来说，并不含有转发时钟（forwarded clock）。相反地，发送端会将时钟通过8b/10b编码来嵌入数据流中，然后接收端将会从数据流中恢复出这个时钟，并用于对输入数据进行锁存。这一过程听起来可能非常神秘，但是其实很简单。在接收端中，PLL电路（Phase-Locked Loop锁相环，如图 2‑5）将输入的比特流作为参考时钟，并将其时序或者相位与一个输出时钟相比较，这个输出时钟是PLL按照指定频率产生的时钟。也就是说PLL自身会产生一个指定频率的输出时钟，然后用比特流作为的参考时钟与自身产生的输出时钟相比较。基于比较的结果，PLL将会升高或者降低输出时钟的频率，直到所比较的双方达到匹配。此时则可以称PLL已锁定，且输出时钟（恢复时钟recovered clock）的频率已经精确地与发送数据的时钟相匹配。PLL将会不断地调整恢复时钟，因此由温度、电压因素对发送端时钟频率造成的影响都会被快速的补偿修正。

关于时钟恢复，有一件需要注意的事情，PLL需要输入端的信号跳变来完成相位比较。如果很长一段时间数据都没有任何跳变，那么PLL的恢复时钟可能会偏离正确的时钟频率。为了避免这种问题，8b/10b编码中的设计目标之一就是要确保比特流中连续的1或者0的数量不能超过5个（想获得更多关于这部分的内容，请参考“8b/10b Encoding”一节）

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image052.jpg)

图 2‑5简单的锁相环图示

一旦时钟被恢复出来，就可以使用它来锁存输入数据流的bit，并将锁存到的结果给到解串器（deserializer）。有时候学生们可能想知道这个恢复时钟能否作为接收端的所有逻辑所使用的工作时钟，但是这个问题的答案是“不行”。一个原因是，接收端不能指望用于恢复出时钟的比特流参考时钟一直存在且活跃，因为当链路的低功耗状态就包括了停止数据传输，此时必然也就无法继续恢复时钟。因此，接收端必须要有自己本地生成的内部时钟。

##### 2.1.2.5   基于数据包的协议体系（Packet-based Protocol）

将并行传输转变为串行传输可以极大的减少数据传输需要的引脚数量。如其他大多数串行传输协议一样，PCIe通过消除了绝大部分原来并行总线中常用的边带控制信号（side-band control signal）来减少了引脚数量。然而，如果没有控制信号来指示被接收的信息的类型，接收端如何知道输入的bit是什么信息呢？因此，在PCIe中，所有的事务在发送时都使用已经定义好的结构，称为数据包。接收端需要找到数据包的边界（即包头和包尾），并且知道这个数据包的被定义的结构是什么样子的（即数据包的模板），然后解析数据包结构来获知它需要执行什么操作。

关于数据包协议体系的详细信息将在“TLP Elements”这一章进行全面讲解，但在本章也可以找到对各种包格式的介绍以及他们各自用途的概述，请见“Data Link Layer”一节。

#### 2.1.3     链路和通道（Links and Lanes）

如先前的内容所提到，两个PCIe设备之间的物理连接被称为一条链路（Link），这个链路由许多通道（Lane）所构成。如图 2‑2，每个通道都含有一对发送差分信号对和一对接收差分信号对。这样的一个通道已经足够用于设备之间的通信，不需要其他额外的信号。

##### 2.1.3.1   可扩展的性能（Scalable Performance）

尽管一个通道（Lane）就可以满足两个设备之间通信的需求，但是使用多的通道的链路可以提升链路的传输性能，这个性能取决于链路的传输速率以及链路宽度。例如，使用多通道链路可以增加每个时钟传输的bit数量，因此这样就可以提升带宽。如表 2‑1所提到，PCIe协议规范支持一条链路中含有的通道数目为2的幂，最多32通道。除了2的幂以外，x12链路也是支持的，这可能是为了支持InfiniBand的x12链路，它是一种较早的串行设计。PCIe这种允许多种链路宽度的特性，使得平台设计者可以在成本和性能之间做出适当的权衡，根据链路中通道的数量轻松地进行增减。

##### 2.1.3.2   灵活的拓扑结构选择（Flexible Topology Options）

一条PCIe链路必须是一个点对点的连接，而不是像PCI一样的共享总线，这是因为PCIe使用的链路速率非常高。由于一条链路只能连接两个接口，因此需要一种扩展连接的方法来构建一个不琐碎的系统（non-trivial system），这里不琐碎的意思是指不过于细碎和冗杂，例如若直接对所有设备都采用直接的两两相连，那么会使得整个系统十分冗杂琐碎。这种需求在PCIe中通过Switches（交换结构）和Bridge来实现，这两者可以灵活的构建系统拓扑——系统中元素之间的连接集。对系统中一个元素的定义以及一些拓扑结构的例子将在下面的小节中给出。

#### 2.1.4     关于PCIe拓扑的一些定义

如图 2‑6展示了一个简单的PCIe拓扑示例，它将有助于我们对本节的一些定义内容进行讲解。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image054.jpg)

图 2‑6 PCIe拓扑示例

##### 2.1.4.1   拓扑特征（Topology Characteristics）

在图的最上方是一个CPU。这里需要指出，CPU被认为是PCIe层次结构的顶端。就像PCI一样，PCIe只允许简单的树结构，这意味着不允许出现循环或者其他复杂的拓扑结构。这样做的原因是为了保持与PCI软件的向后兼容性，因为PCI软件使用简单的配置方法来记录拓扑结构，它并不支持复杂的环境。

为了保持这种向后兼容性，软件必须能够以与从前一样方式产生配置周期，并且总线拓扑结构也必须与以前一样。因此，软件希望找到的所有配置寄存器依然存在，而且它们的行为方式等也依然与从前相同。我们将稍晚些在回过头来讨论这一块的内容，因为我们要先定义一些术语概念。

##### 2.1.4.2   Root Complex（RC 根组件）

CPU与PCIe总线们支架可能包含一系列的组件（处理器接口，DRAM接口等等），甚至是包含多个芯片。将这些组件合起来，称这一组组件为Root Complex（根组件，简称为RC或者Root）。RC存在于PCI倒转的树状拓扑的“根部”，并代表CPU与系统的其余部分通信。但是，PCIe协议规范并没有很仔细的对RC进行定义，而是给出了一个RC必需功能与可选功能的列表。从广义上说，Root Complex可以被理解为系统CPU与PCIe拓扑之间的接口，这个PCIe端口，即RC，在配置空间中被标记为“根端口（Root Port）”。

##### 2.1.4.3   Switches and Bridge（交换结构与桥）

Switch提供了扇出以及聚合能力，使得单个PCIe端口上可以连接更多的设备。它扮演数据包路由器的角色，可以根据所给数据包的地址或者其他路由信息来识别这个数据包要走哪条路径。

Bridge提供了一个通往其他总线的接口，例如PCI或者PCI-X，甚至也可以是其他的PCIe总线。图 2‑6中展示的Bridge有时被称为“forward bridge（前向Bridge）”，它使得一个旧的PCI或者PCI-X板卡可以插入一个新系统中（PCIe系统）。与forward bridge相反类型的Bridge称为“reserve bridge（反向Bridge）”，它使得新的PCIe板卡可以插入到旧的PCI系统中。

##### 2.1.4.4   Native PCIe Endpoints and Legacy PCIe Endpoints（原生与遗留EP）

Endpoints（EP，端点）是PCIe拓扑中的既不是Switch也不是Bridge的设备，它们可以作为总线上事务的Initiator也可以作为事务的Completers。EP存在于树状拓扑的分支的底部，仅实现一个上行端口（面向Root），这里的上行指的是拓扑结构的向上，表示EP已经是树的分支的端点，不再产生下级分支。相比之下，一个Switch可以有几个下行端口。设计用于老式总线（例如PCI-X）的设备如今拥有了可供它们使用的PCIe接口，这种PCIe接口将在配置寄存器中将自身标识为“Legacy PCIe Endpoints（遗留PCIe端点）”。它们使用了在PCIe设计中被禁止的东西，例如IO空间、支持IO事务以及支持锁定请求。与之相反，“Native PCIe Endpoints（原生PCIe端点）”是从一开始就被设计用来在PCIe系统中使用的，这区别于在旧的PCI设备上添加PCIe接口。原生PCIe端点设备是内存映射设备（MMIO设备）。

##### 2.1.4.5   软件兼容特性

一种保持软件兼容性的方法是保持EP和Bridge的配置头（configuration header）与PCI一致，如图 2‑7所示。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image056.jpg)

图 2‑7 配置头

有一点不同的是在现在PCIe中Bridge经常会被聚合进Switch和Root，但是遗留的PCI软件是无法发现这种区别的，它只把这些内含Bridge的设备简单的当做Bridge。在这里我们先对一些概念进行熟悉，所以我们不会在这里讨论这些寄存器的细节。配置是一个相当大的主题，关于它的具体介绍将在“Configuration Overview”这一节进行。

为了举例说明PCIe系统在软件中的展现形式，我们可以参考图 2‑8中所展示的拓扑结构的例子。像之前所说的一样，RC位于整个层次结构的顶端。RC自身的内部可以非常复杂，但是它通常会实现一个内部总线结构以及一些桥接结构，以便将拓扑扇出到几个端口。RC的内部总线将被配置软件视作PCI总线0，且RC上这几个PCIe端口将被视作是PCI-to-PCI Bridge。这种内部结构其实并不是一个实际的PCI总线，但是它在软件中看起来就是这种结构。因为这个总线是在RC内部的，所以它实际的逻辑设计并不需要遵循任何的标准，这里是可以由供应商来进行自定义的。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image058.jpg)

图 2‑8拓扑结构示例

类似的，PCIe Switch（交换结构）的内部结构对于软件来说就是简单地几个Bridge共享一条公共总线，如图 2‑9所示。这样做的主要优点就是使得事务的路由方法能够与PCI一致。枚举，这是配置软件用来发现系统拓扑结构并分配总线号和系统资源的过程，它的工作方式也与PCI中相同。在稍后的内容中我们将通过一些例子来讲述枚举是如何工作的。一旦枚举完成，系统中的总线号就将按照图 2‑9所示的方式进行分配。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image060.jpg)

图 2‑9系统枚举结果的示例

##### 2.1.4.6   系统示例

如图 2‑10中举例说明了一个基于PCIe的系统，它被设计用于一些低成本应用比如消费品台式计算机。有一些PCIe端口实现时附带了一些板卡插槽，但是基本的框架与老式的PCI系统并没有太大区别。

相比之下，如图 2‑11所示的高端的服务器系统展示了一些内置在系统的用于连接其他网络的接口。在早期PCIe中，有人曾考虑将其作为一个网络来运行，用来取代那些旧的模型。毕竟，如果PCIe在大体上是其他网络协议的简化版本，或许它也能满足所有需求。由于各种各样的原因，这一概念从来没有真正得到大力发展，基于PCIe的系统通常仍然需要使用其他的协议来连接到外部网络。

这也给了我们一个机会来重新审视一个问题，RC是由什么组成的。在这个例子中，被标识为“英特尔处理器”的方块包含了许多组件，大多数现代的CPU架构都是如此。这个处理器包含了一个用于访问图形设备（例如显卡）的x16 PCIe端口，以及两条DRAM通道，这两条DRAM通道意味着内存控制器以及一些路由逻辑已经被集成到CPU封装（CPU Package）中。总的来说，这些资源通常被称为“非核心”资源，这样的称呼用于将他们与CPU封装中的几个CPU核心（CPU Core）区分开来。此前，我们描述过Root是CPU与PCIe拓扑相连接的接口，这意味着在CPU封装中必须含有Root的一部分。正如图 2‑11中虚线所框出的，Root由多个封装的一部分共同组成。这种Root的组成方式可能将会是未来很长一段时间内的系统的设计方式。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image062.jpg)

图 2‑10 低成本的PCIe系统

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image064.jpg)

图 2‑11服务器PCIe系统

 

### 2.2 设备层次介绍（Introduction to Device Layers）

PCIe定义了一种分层的体系结构，如图 2‑12是这种分层体系结构的示意图。我们可以认为这些层在逻辑上是相互独立的部分，因为他们各自都有一个用于发出信息流的发送端和一个接收信息流的接收端。这种分层的设计方法对硬件设计者来说有不少的优点，因为如果在设计中对逻辑进行了仔细的划分，那么就可以在以后升级到新的协议规范版本时仅改变原设计中的某一层即可，而不会影响或者变动其它层。虽然如此，但是需要注意的是，这些层只是定义了接口的工作职责，而实际设计并不要求为了符合规范而将设计按照层级来分成几个部分，即分层的概念更偏重于在逻辑层面，不强制要求划分的很严格。本节的目的在于描述各个层的功能职责，以及描述完成一次数据传输时的事件流程。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image066.jpg)

图 2‑12 PCI Express设备层次示意图

图 2‑12所展示的PCIe设备内部层次包括：

l 设备核（Device Core）以及它与事务层（Transaction Layer）的接口。设备核实现设备的主要功能。如果设备是一个EP（端点），那么它最多可以包含8个Function（功能），每个Function实现自己的配置空间。如果设备是一个Switch（交换结构），那么它的Switch核是由数据包路由逻辑和为了实现路由的内部总线而构成。如果设备是一个RC（Root根设备），那么他的Root核会实现一个虚拟的PCI总线0，在这个虚拟的PCI总线0中存在着所有的芯片组嵌入式EP，以及存在着虚拟Bridge。

l 事务层（Transaction Layer）。事务层负责在发送端产生Transaction Layer Packet（TLP，事务层包），在接收端对TLP进行译码。这一层也负责QoS功能（Quality of Service，服务质量）、流量控制功能（Flow Control）以及事务排序功能（Transaction Ordering）。所有的这四个事务层的功能将在本书的第二部分Part Two进行讲解。

l 数据链路层（Data Link Layer）。数据链路层负责在发送端产生Data Link Layer Packet（DLLP，数据链路层包），在接收端对DLLP进行译码。这一层也负责链路错误检测以及修正（Link error detection and correction），这个数据链路层功能被称为Ack/Nak协议。这两个数据链路层功能会在本书的第三部分Part Three进行讲解。

l 物理层（Physical Layer）。物理层负责在发送端产生Ordered-set Packet（命令集包），在接收端对Ordered-set进行译码。这一层将处理上述三种类型的包（TLP、DLLP、Ordered-set）在物理链路上的发送与接收。数据包在发送端要经过byte striping logic（字节条带化逻辑，主要在多lane时体现作用）、Scrambler（扰码器）、8b/10b encoder（对于Gen1/Gen）或是128b/130b编码器（对于Gen3）以及数据包并串转换模块的处理。最终数据包以训练后的链路速率在所有Lane（通道）上按照时钟以差分形式输出。在物理层的接收端，数据包的处理包括串行的接收差分信号所传输的经过编码的bit，将其转换为数字信号形式，然后将输入比特流做串并转换。完成这个操作（对输入数据的采样）所基于的时钟是来源于CDR（Clock and Data Recovery，时钟数据恢复）电路所提供的恢复时钟。接收下来的数据包要经过elastic buffers（弹性缓存）、8b/10b解码器（对于Gen1/Gen2）或者128b/130b解码器（对于Gen3）、de-scramblers（解扰器）以及byte un-striping logic（字节反条带化逻辑）。最终，物理层的LTSSM（Link Training and Status State Machine，链路训练与状态 状态机）负责进行链路初始化以及训练。所有这些物理层功能将在本书的第四部分Part Four进行讲解。

每个PCIe接口都支持这些层的功能，包括Switch端口，如图 2‑13所示。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image068.jpg)

图 2‑13 Switch端口的层次

在早期大家经常会产生一个疑问，那就是一个Switch端口是否还需要实现所有的层呢，毕竟它通常只用于转发数据包。答案是需要，因为对包的内容进行解析来确定它们的路由是需要查看数据包的内部细节的，而这件事情是在事务层（Transaction Layer）中做的。

原则上，本设备上的每个层都与链路对端设备的各自对应层进行通信，例如本设备的事务层与对端的事务层通信、本设备的数据链路层与对端的数据链路层通信。上面的两层通过在数据包内添加一串特定的bit信息，产生一个接收端的对应层可以识别的字段样式来实现对应层之间的通信。数据包通过其他层的转发，从而做到到达或者离开链路。物理层也直接与另一个设备中的物理层通信，但是方式与此不同。

在我们讲的更深入之前，让我们先大致了解一下这些层是如何交互的。从广义上说，设备所发出的请求包或者完成包是在事务层进行组包的，组包所用到的信息是由device core所提供的，有时我们将device core称为Software Layer软件层（尽管协议规范中并没有使用这一术语）。它提供的组包信息通常包括期望的命令类型、目标设备的地址、请求的属性特征等等。刚组好的数据包会被存入一个buffer称为Virtual Channel（虚拟通道），直到这个包可以被发往下一个层级。当这个包被向下发给数据链路层后，数据链路层会在数据包中加入额外的信息以供对端的接收方进行错误检查，而且这个数据包会在本地储存下来，这样我们就可以在对端检测到传输出错时重新发送这个数据包。当数据包到达物理层后，它被编码，并使用链路上的所有可用的通道、以差分信号的形式进行传输。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image070.jpg)

图 2‑14 PCIe设备层次的详细框图

接收端对物理层的输入bit进行解码，检查本层级所能发现的错误，如果没有检查到错误那么就将接收下来的数据包向上转发给数据链路层。在数据链路层，数据包进行不同于物理层的错误检查，如果没有发现错误那么就向上转发给事务层。这个数据包在事务层被缓存和错误检查，并被拆包拆成各原始信息（命令、属性等等），这样就可以这些原始信息内容呈交给device core，这里的device core是这次数据传输的接收端设备的device core。接下来，让我们更深入的探索每一个层需要怎么做才能完成上述这样的操作过程，这个操作过程在图 2‑14进行了展示。我们从最顶端开始。

#### 2.2.1     设备核/软件层（Device Core/Software Layer）



Device Core是一个设备的核心功能，例如网络接口或是硬盘驱动控制器。它并不是PCIe协议规范中所定义的一个层级，但是我们可以把它当做一个PCIe的层级，这是因为它位于事务层的上方，而且它是所有请求（Request）的源头或是目的地。它为事务层提供了需要发送的请求信息，其中的信息包括事务类型、地址、需要传输的数据量等等。当事务层接收到输入数据包时，它也是事务层向上转发输入数据包信息的目的地。

#### 2.2.2     事务层（Transaction Layer）

为了响应（response）来自软件层的请求（request），事务层生成出站数据包（outbound packet）。它也会检查入站数据包（inbound packet），并将入站数据包内包含的信息向上转发给软件层。事务层支持non-posted（非投递式）请求的拆分事务协议（Split Transaction Protocol，前面在PCI-X的介绍中提到过），并将入站完成包（Completion）与先前传输的出站non-posted请求包关联起来，即知道这个完成包是对应到哪个non-posted请求包。事务层所处理的事务使用的数据包种类为TLP（Transaction Layer Packet），TLP可以分为四个请求种类：

\1.    Memory

\2.    IO

\3.    Configuration

\4.    Message

前三种（Memory、IO、Configuration）在PCI和PCI-X中就已经得到支持，但是Message是PCIe中的一个新的请求种类。一个请求包向目标设备传送命令，目标设备作为响应请求而发回的一个或多个完成包，这二者组合起来就是对一个事务的定义，即一个事务由一个请求包以及所有返回的完成包共同组成。如表 2‑2列出了PCIe请求的类型。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image072.jpg)

表 2‑2 PCIe请求类型

这些请求还可以被归为两类，如上表的右边一列：**non-posted**和**posted**。对于non-posted请求，Requester首先向Completer发送一个请求数据包，Completer应该产生一个完成包作为响应。读者们应该认出来了，这就是从PCI-X那里继承来的拆分事务协议（split transaction protocol）。例如，所有的读请求都是non-posted的，因为这个读请求所请求的数据需要通过完成包来返回给Requester。可能令你感到出乎意料，IO写请求和Configuration写请求也是non-posted的。尽管它们在发送给目标设备的命令中就已经包含了要写入目标设备的数据，但是这两种请求依然需要目标设备在写入完成后返回完成包，以此来让Requester确认数据被正确无误的写入到目的设备中。

与上述的请求相反地，Memory Write请求（Mwr）和Message请求都是posted的，这意味着Completer在完成这些请求后不需要向Requester返回完成包TLP。Posted事务对整体性能提升是有好处的，因为Requester不需要等待响应，也不需要承担对完成包进行处理的额外负担。这里做出的取舍就是Requester无法得到写请求是否被正确无误的完成的反馈信息。Posted这种操作行为继承自PCI，它依然被认为是一个不错的操作方法，虽然无法得到反馈信息，但是发生错误的可能性比较小并且使用posted得到的性能提升也比较明显。需要注意的是，尽管它们要求返回完成包，Posted写操作仍然要参与数据链路层的Ack/Nak协议，以此来保证较为可靠的数据包传输。关于这一点的更多内容，请见Chapter 10“Ack/Nak Protocol”。

##### 2.2.2.1   TLP基础内容

PCIe请求包和完成包的包类型被罗列在表 2‑3中。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image074.jpg)

表 2‑3 PCIe TLP类型

TLPs起始于发送方的事务层，终止于接收方的事务层，如图 2‑15所示。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image076.jpg)

图 2‑15 TLP的起点与目的地

当TLP途经发送方的数据链路层以及物理层时，这两层分别会向数据包中添加一些信息，接收方的数据链路层和物理层会分别根据发送方对应层所添加的信息来进行校验，以此确认数据包是否在链路传输中依旧保持正确没有出错。

###### l TLP组包（TLP Packet Assembly）

如展示了一个封装完成的TLP的各个部分是如何在链路上进行传输的，我们可以从中发现这个数据包中的不同部分是分别由不同的层来添加的。为了更容易看出这个数据包是怎么构成的，我们将TLP的不同部分用不同的颜色进行标识，以此来表示对应的部分是由哪一层添加的：红色代表事务层，蓝色代表数据链路层，绿色代表物理层。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image078.jpg)

图 2‑16 TLP的组包形式

Device Core负责将组成TLP的核心部分的信息发送给事务层，即上图中的Header和Data。每个TLP都会有一个Header（头部），而有些TLP会没有数据部分，例如读请求。事务层还可以选择添加ECRC（End-to-End CRC）字段，这个ECRC由事务层进行计算并附加在数据包的后面。CRC的意思是Cyclic Redundancy Check/Code（循环冗余校验/循环冗余校验码），它被几乎所有的串行传输架构所使用，原因很简单，它实现起来比较简单同时又能提供很强的错误检测功能。CRC还可以检测“突发错误（burst error）”，即一串重复的错误bit，这一串bit的长度取决于CRC的长度（对于PCIe来说是32bits）。因为这种类型的错误在发送一长串bit时会遇到，因此CRC的这种特性非常适用于串行传输。ECRC字段区域在通过发送方和接收方之间的任何服务点（service point，通常指的是Switch或者Root端口这些有TLP路由功能的地方）时都不改变，这使得目的端可以用它来验证在整个传输过程中都没有发生错误。

现在说说TLP的传输，TLP的核心部分由事务层转发至数据链路层，数据链路层负责在TLP中添加一个序列号（Sequence Number）和另外一个CRC字段区域称为链路CRC（LCRC，Link CRC）。LCRC被对端接收方用来进行错误检查，并将链路上传输的每个数据包的检查结果都汇报给发送方。善于思考的读者可能会有疑问，如果LCRC已经证明了这次链路传输是无差错的，而LCRC又是数据包必需含有的字段，那么ECRC还有什么作用呢？这个疑问的回答是，还使用ECRC是因为还有一个地方的传输错误没有被检查，那就是负责路由数据包的设备内部。一个数据包到达一个端口，并进行错误检查，也进行路由检查，然后当它被从另一个端口发出时会计算出一个新的LCRC值并添加在数据包中，新的LCRC会取代老的LCRC。内部端口之间的转发可能会遇到PCIe协议没有检查到的错误，这时候就需要ECRC来发挥它的作用了。这里可以理解为，当TLP到达Switch的一个接收端口时，Switch会在接收端口对其LCRC进行校验，然后根据路由信息对数据包进行转发，但是这个转发过程是不对LCRC进行校验的，直到转发到对应的输出端口，然后在输出端口给数据包加上新的LCRC后发送出去，不难发现因为在内部转发过程中不对LCRC进行校验，因此若转发过程中出现了错误则就需要通过更内层封装的ECRC来进行校验了，否则内部转发过程是否出错将无从可知。

最后，数据链路层封装好的数据包被转发给物理层，物理层将其他一些字符添加到数据包中，这些字符可以让接收方知道接下来将会接收什么（例如标识一个数据包的开始或结尾）。对于前两代的PCIe，物理层将在数据包的头和尾添加控制字符。而在第三代PCIe中不再使用控制字符，而是在数据包中添加一些额外的bit来提供关于数据包的信息。经过这些处理后，数据包被编码（8b/10b for Gen1/Gen2，128b/130b for Gen3），然后在链路上所有可用通道中以差分形式传输。

###### l TLP拆包（TLP Packet Disassembly）

当对端的接收方看到了输入的比特流时，它需要对此前组包时添加的哪些部分进行识别和剥除，这样就能恢复出发送方设备的Device Core的原始请求信息。如图 2‑17所示，物理层将会确认当前比特流中是否存在正确的“起始”、“结束”或者其他字符，并将它们剥除，然后将剥除了这些字符后的TLP转发给数据链路层。数据链路层将首先进行LCRC以及序列号（Sequence Number）的错误校验。如果并未发现错误，那么数据链路层将会把LCRC和序列号从TLP剥除，并将TLP转发给事务层。如果接收方是一个Switch，那么将在事务层对这个数据包进行解析评估，从它的数据包头中找到路由信息来确定这个数据包要被转发到哪一个端口。即使这个Switch并不是TLP最终的目的地，它也可以对这个TLP进行ECRC校验以及在发现错误时进行ECRC错误汇报。但是， Switch不能更改这个TLP中的ECRC，这是希望最终的目标设备也可以检测到这个ECRC错误。

如果目标设备有能力并且启用了ECRC校验的功能，那么它将对ECRC进行错误校验。若TLP到达了最终的目的设备，而且校验无错，那么在事务层中将把这个ECRC字段剥除，使得整个数据包只剩下Header和数据部分，并将这些剩余部分转发给软件层。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image080.jpg)

图 2‑17 TLP的拆包形式

##### 2.2.2.2   Non-Posted事务

###### l 普通读（Ordinary Reads）

如图 2‑18展示了一个Memory Read请求，这个请求由EP发往系统memory。在Memory Read请求TLP中有一个很重要的部分，那就是目标地址（关于更多有关TLP内容的详细讲解，请见Chapter 5 “TLP Element”）。一个Memory请求的地址可以是32bit或者64bit的，这也决定了数据包的路由方式。在这个例子中，这个请求事务通过两个Switch的路由后向上转发给了目标设备，例子中的目标设备为RC。当RC对请求包进行译码，并识别出了数据包中的地址指向了系统memory，它将从那段系统memory中取出EP所请求的数据。为了将这些从memory中取出来的数据返回给Requester，RC端口的事务层将产生足够多的完成包（Completion），这些数量的完成包足以将Requester所请求的全部数据都返回回去。PCIe中规定一个数据包中数据荷载（data payload）最大为4KB，但是实际设计的设备中一般会使用的payload会比4KB小，因此需要好几个完成包才能足够将比较大量的数据返回给Requester。

这些完成包内也包含了路由信息，用于将它们指引回到Requester，这是因为Requester在原先的请求包中就附带了需要返回的地址的信息，所以Completer就可以将这个地址用来放在完成包中作为完成包的路由信息。这个“返回地址”其实很简单，它就是PCI中定义的Device ID，这个ID由三个东西组成：Requester所属PCI总线在系统中的PCI总线号、Requester在所属PCI总线上的设备号、Requester在所属设备中的Function号。这样的总线号、设备号、Function号组合起来的信息（有时被缩写为BDF，Bus、Device、Function）就是完成包用来返回到Requester的路由信息。正如PCI-X所做的那样，Requester可以同时有多个正在进行的拆分事务（split transaction），它必须能够将输入的完成包和正确的请求关联起来。为了便于实现这一点，Requester在原先的请求包中加入了一个值称为Tag，这个Tag对于每一个请求而言都是独一无二的，也就是说每一个未完成的请求都有一个与其他未完成的请求不同的Tag号。Completer会将这个事务的Tag拷贝进完成包中，这样Requester就可以快速地通过完成包中的Tag来将这个完成包与正确的请求关联起来，也就是找到了这个完成包是用来服务哪个请求的。最后还要说一点，Completer还可以设置完成包中的完成状态字段中的bit，以此来表示出事务的错误情况。这至少可以让Requester对哪里可能出错了有一个大致的了解。Requester如何去处理大多数的错误，这个事情一般是由软件来决定，这并不在PCIe协议规范的范围内。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image082.jpg)

图 2‑18 Non-Posted Read的示例

###### l 锁定读（Locked Reads）

Locked Memory Read这种读请求是为了支持一种叫做原子读-修改-写操作，所谓原子操作就是一种不可中断的事务，处理器使用这种事务来进行测试以及设置信号标（semaphore）等任务。当测试和设置信号标正在进行中时，不允许其他对信号标的访问发生，不予许发生竞争的情况。为了避免发生竞争情况，处理器使用一个锁定指示符（lock indicator）（例如并行总线前段的一个单独的pin），来阻止总线上的其他事务，直到这个锁定的事务完成。接下来是对这个主题内容的一个高层次介绍。有关被锁定事务的更多信息，请参阅附录D，“Appendix D：Locked Transactions”。

回顾一下历史，在PCI的早期，协议规范的制定者们预期到PCI将会实际取代处理器总线。因此，PCI协议规范中要求总线要能支持处理器要完成的事情，比如锁定事务。然而，PCI很少以这种方式使用，最终，这种处理器总线支持的东西大部分都被丢弃了。不过，锁定周期依然存在，用以支持一些特殊情况，在更进一步的PCIe的发展中也保留了它，以实现对一些遗留事务的支持。也许是为了加速向PCIe的迁移，在新的PCIe设备中进制接收锁定请求（locked request），只有那些遗留设备才能够使用它。在图 2‑19所示的例子中，一个Requester发出了一个MRdLk（Memory Read Locked）来发起事务。根据定义这样的请求仅允许来自CPU，因此在PCIe中仅有RC的端口可以发起这种事务。

这个locked request使用目标memory地址作为路由信息，并最终到达了遗留设备EP（Legacy Endpoint）。当数据包经过沿途的每个路由设备时（称为服务点），数据包的出口端口被锁定，这意味着在被解锁之前这条路径都不能通过其他的数据包。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image084.jpg)

图 2‑19 Non-Posted Locked Read事务协议

当Completer接收到这个请求包并将其内容进行译码，它将收集请求所需要的数据，并使用这些数据产生一个或多个Locked完成包。这些完成包将会被路由回到Requester，使用的路由信息是Requester ID，完成包路径上的出口端口也会被锁定。

如果Completer遇到了一些问题，无法正常响应请求，那么它将向Requester返回一个不含有数据的Lock完成包（正常的读请求完成包应该含有要返回的数据，而这里不包含数据那么我们就知道出现了一些问题），并使用完成包的状态指示区域来标识发生的错误的一些信息。Requester接收到这样的完成包就明白了这个Lock Request失败了，它就会取消这个事务的执行，然后由软件来决定接下来要做什么。

###### l IO和配置写（IO and Configuration Writes）

如图 2‑20中展示了一个Non-Posted IO写事务。与Locked request相类似，这样的一套IO操作流程的目的设备只能是一个遗留EP。请求包使用IO地址作为路由信息在Switch中被路由转发，直到它到达目标EP。当Completer接收到这个写请求包，它接收请求包内的数据并返回一个单独的不含有数据的完成包，以此来确认自己收到了这个写请求包。完成包中的状态标识区域将会指示出是否发生了错误，如果发生了错误，Requester的软件需要对错误进行处理。

如果完成包中显示并未出现错误，那么Requester就认为写数据被成功的送达目标设备并成功写入，可以允许针对这一个Completer的指令序列继续向下执行。也就是说，在Requester中有一系列的针对Completer的指令，当知道这一个IO写请求完成了之后，允许执行指令序列中的下一个指令，即想表达的是需要确认这个写请求成功了才允许继续执行下一条指令。这很好的总结了Non-Posted写的目的：不同于一个Memory写（MWr是Posted的），Non-Posted写不能仅仅知道数据被送往了目的方，还必须要知道数据真的到达了目的方才行，否则在逻辑上不能继续执行下一步。又如同Locked操作一样，Non-Posted写请求只能由处理器发出。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image086.jpg)

图 2‑20 Non-Posted Write事务协议

##### 2.2.2.3   Posted写事务

###### l Memory写（Memory Writes）

Memory写请求必然是Posted类型的，它不需要返回完成包。一旦这种写请求包被发出，Requester不需要等到任何反馈就可以继续去进行下一条请求，不需要在返回完成包这件事情上花费时间和带宽。因此，Posted写会比Non-Posted写更加快速且高效，并很好的提升了系统的性能。如图 2‑21所示，请求包使用memory地址作为路由信息在系统中进行路由转发，并最终到达Completer。一旦链路成功的将这个请求发送过去（这里的成功是指把数据包传输了过去，而不需要得到Completer的成功确认），这个事务在链路上就已经结束了，链路上就可以继续传输其他的数据包了。最终，Completer接收写请求包中的数据，这个事务才真正完成。当然，这种事务执行方法在提升效率的同时也舍弃了一些东西，因为Completer不需要发送完成包，所以这也意味着它无法将错误报告给Requester。如果Completer发生了错误，那么它可以记录下这个错误，并向RC发一个Message来通知系统软件这些情况，但是Requester对这些是完全不知情的。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image088.jpg)

图 2‑21 Posted Memory Write事务协议

###### l Message写（Message Writes）

十分有趣的是，Message不同于我们前面所说的那些请求事务，它有好几种路由方法（前面的都是通过memory地址、IO地址中的一种），在Message内部专门有一个区域来标识使用的是哪一种路由方式。例如，有一些Message是Posted写请求，其目的方为特定的Completer；有一些是RC向所有EP广播的请求；还有一些是EP发出的要自动路由到RC的请求。想要学习更多关于不同的路由类型的内容，请参阅Chapter 4“Address Space & Transaction Routing”。

Message在PCIe中很有用，它可以使得PCIe达到减少引脚数量的设计目标。它使得PCIe不再需要边带信号（side-band signals），而在PCI中则是需要用边带信号来报告中断、功耗管理事项以及错误信息，而在PCIe中使用Message则可以将这些信息使用数据包来进行报告，数据包是直接在一般数据路径上传输的，因此不再需要额外的边带信号。

##### 2.2.2.4   QoS服务质量（Quality of Service）

PCIe从一开始就被设计为能够支持时间敏感事务，例如流视频、音频应用程序，对于这些应用程序来说数据必须被及时传输才能保证数据有效。这被称为提供了QoS，它是通过添加一些东西来实现的。首先，每个数据包都被软件分配了一个优先级，这个优先级是通过设置数据包内的一个3bit的字段区域来进行标识的，称之为流量类型Traffic Class（TC）。一般来说，给一个数据包分配一个编号较大的TC表示希望给与这个数据包一个更高的优先级。第二，使用多缓冲区，称为虚拟通道Virtual Channels（VC），将其构建在硬件的每一个端口中，数据包会根据其TC值也就是优先级，来被放入响应的虚拟通道中（相应的缓存区中）。第三，由于现在对于一个端口来说，在一个时刻将会有多个缓冲区都存在可以进行传输的数据包，因此需要有对VC进行选择的仲裁逻辑。最后，Switch必须在相竞争的各个输入端口间做出选择，以便访问相应端口的VC。这一步被称为端口仲裁，它可以由硬件来进行分配或是由软件来进行编程配置。这与第三点的不同之处在于，第三点是在一个端口内对多个VC进行仲裁选择，而端口仲裁是指各个端口已经选择好了此次访问的VC，需要由Switch仲裁选择访问哪一个端口。所有的这些硬件部件都必须要能够支持系统对数据包进行优先级排序。如果一个这样的系统被正确的编程配置，它可以为给定的路径提供有保障的服务。

为了用图片来解释这个概念，请见图 2‑22，其中的视频摄像头以及SCSI设备都需要向系统DRAM发送数据。这二者的不同之处在于，摄像头是对时效性要求严格的，如果传输路径无法满足摄像头的带宽，那么将出现丢帧的现象。因此系统需要保障摄像头所需要的最小带宽，否则它捕捉的视频画面将会出现不稳定。与此同时，SCSI数据需要正确无误的进行传输，但是对于它来说传输需要的时间长短并不是那么重要。显然，当视频数据和SCSI数据同时需要被发送时，视频数据流应当具有更高的优先级。QoS指的是系统的一种能力，是系统为数据包分配不同优先级并将这些数据包按照确定性的延时、带宽在系统拓扑中进行路由的能力。想了解更多关于QoS的细节，请参阅Chapter 7“Quality of Service”。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image090.jpg)

图 2‑22 QoS示例

##### 2.2.2.5   事务排序（Transaction Ordering）

在一个虚拟通道VC内，数据包一般全都按照它们到达的先后顺序进行排序和移动，但是也有例外情况。PCIe协议继承了PCI的事务排序模型，包括支持PCI-X中加入的宽松排序（relaxed-ordering）。这些排序规则保证了使用相同TC（Traffic Class）的数据包都会按照正确的顺序在拓扑结构中被路由转发，防止潜在的死锁或者活锁情况。需要注意的一点是，由于排序规则仅应用于VC，且使用不同TC的数据包一般不会被划分进同一个VC，因此使用不同的TC的数据包在软件看来就不存在排序关系。这种排序在事务层中的VC中维护。

##### 2.2.2.6   流量控制（Flow Control）

串行传输所使用的一个典型协议是，要求发送方仅在对端有足够的缓冲区接收时才发送数据包。这样的规定删去了总线上的性能浪费操作事件，比如PCI中允许进行的断开（disconnect）与重试（retry），这使得这类问题在传输中得到消除。这样做的代价是，接收方必须足够频繁的报告它的可用缓冲区空间来避免不必要的传输停顿，而且这样的报告也需要占用接收方自己的一点带宽。在PCIe中，这个可用缓冲区空间的报告是由DLLP（Data Link Layer Packet）来完成的，我们将在下一节讲到DLLP。这里不使用TLP的原因是为了避免可能出现的死锁现象，如果使用TLP则可能出现，例如一个设备A作为发送方需要对接收方B的可用缓冲区空间进行更新，但是当A的接收缓冲区满导致它又无法接收来自B的可用缓冲区报告，这样就出现了一个死循环。而DLLP可以不管缓冲区状态就进行收发，这样就避免了死锁的问题。这样的流量控制协议由硬件级进行自动管理，对于软件来说是透明的。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image092.jpg)

图 2‑23流量控制基础操作

如图 2‑23所示，接收方内的VC Buffer内缓存了接收到的TLP。接收方将自己的缓冲区大小通过Flow Control DLLP（流量控制DLLP）来告知发送方。发送方将会跟踪接收方的可用缓冲区空间的值，并且不允许发出大于这个可用空间的数据包。当接收方对缓冲区中的TLP进行了处理，并将这个TLP移出了缓冲区，此时缓冲区中就空闲出了新的可用空间，那么接收方将会定期的发送Flow Control Update DLLP（流量控制更新DLLP）来保持发送方能够获取到最新的可用空间的值。想了解更多关于这方面的内容，请见Chapter 6“Flow Control”。

#### 2.2.3     数据链路层（Data Link Layer）

数据链路层这一层的逻辑是用来负责链路管理的，它主要表现为3个功能：TLP错误纠正、流量控制以及一些链路电源管理。它是通过如图 2‑24所示的DLLP（Data Link Layer Packet）来完成这些功能的。

##### 2.2.3.1   DLLPs数据链路层包（Data Link Layer Packet）

DLLP是一种在位于同一条链路上的本端设备的数据链路层与对端设备的数据链路层之间传输的数据包。事务层对这些数据包并无感知，它们只在两个相临近的设备之间传输，不会被路由到任何其他地方。DLLP通常要小于TLP，大部分情况下DLLP只有8Bytes，这是一件好事，因为DLLP的大小也反应着维护链路所需要的开销。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image094.jpg)

图 2‑24 DLLP的起始地和目的地

###### l DLLP组包（DLLP Assembly）

如图 2‑24所示，一个DLLP起源于发送方的数据链路层，并结束于接收方的数据链路层。在这个DLLP中由发送方给DLLP Core加上了一个16bit CRC，用来供接收方进行错误校验。在加完CRC之后，这个DLLP被转发至物理层，在这一层里给DLLP加上包起始字符和包结束字符（这种方法是Gen1和Gen2的方法，Gen3不再使用），并对加完两种字符的DLLP进行编码，然后通过链路上的所有通道（Lane）进行差分传输。

###### l DLLP拆包（DLLP Disassembly）

当接收方的物理层接收到了一个DLLP，这个DLLP的比特流将被译码并剥去包起始字符与包结束字符。剩余的DLLP部分被转发至数据链路层，在这里进行CRC错误校验，并根据包的内容执行相应的操作。接收方的数据链路层就已经是DLLP的最终目的地，它不会被继续向上呈交给事务层。

##### 2.2.3.2   Ack/Nak协议（Ack/Nak Protocol）

如图 2‑25所示是一种基于硬件的自动重试机制（hardware-based automatic retry mechanism）。如图 2‑26所示，每一个被发送方发出的TLP都被加上了LCRC与序列号（Sequence Number），在接收方会对这两个信息进行检查。发送方的重传Buffer（replay buffer）保存着每个被发送的TLP的副本，直到对端确认成功收到了这个TLP。这个确认成功收到的过程是通过接收方发送Ack DLLP来实现的，在这个Ack DLLP中包含有接收方成功接收的上一个TLP的序列号（Sequence Number）。当发送方收到这个Ack DLLP，它将会把里面Sequence Number所对应的TLP、以及这个TLP之前的所有TLP，都从重传Buffer内清除。

如果接收方检测到了一个TLP错误，它将会把这个TLP丢弃，并向发送方返回一个Nak DLLP，以期望发送方能对未确认成功接收的TLP进行重传，并通过重传获得一个完好的TLP。由于错误检测通常是十分迅速的操作，因此从错误检测开始到发起重传的时间也比较短，这样就可以在较短的时间内纠正发生的错误。这一操作过程被称为Ack/Nak协议（Acknowledge/NotAcknowledge）。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image096.jpg)

图 2‑25 数据链路层重传机制

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image098.jpg)

图 2‑26 在数据链路层中的TLP与DLLP的包结构

一个DLLP的基础形式如图 2‑26所示，它由4Byte的DLLP类型域（DLLP Type field）以及2Byte CRC组成，其中DLLP类型域中包含了一些其他的信息。

如图 2‑27所示，它展示了一个memory read TLP通过Switch的示例。一般来说，这个过程的步骤如下：

**1.**    **步骤1a****：**Requester发送一个memory read请求，并在自身的重传Buffer（replay buffer）中也保存一个请求包的副本。Switch接收这个MRd TLP（Memory Read TLP，以后简写为MRd TLP），并校验LCRC以及序列号（Sequence Number）。

**步骤1b****：**校验未发现出错，因此Switch向Requester返回一个Ack DLLP。作为响应，Requester将其重传Buffer中保存的这个MRd TLP的副本丢弃。

**2.**    **步骤2a****：**Switch将这个MRd TLP转发到正确的输出端口，使用的路由信息是这个MRd TLP的memory地址，在转发出去的同时，Switch会在这个输出端口的重传Buffer内保存一份这个TLP的副本。Completer接收到这个MRd TLP并对其进行错误校验。

**步骤2b****：**校验未发现出错，因此Completer向Switch返回一个Ack DLLP。作为响应，Switch之前输出这个TLP的输出端口会将其重传Buffer中保存的这个MRd TLP的副本丢弃。

**3.**    **步骤3a****：**作为这个请求的最终目的地，Completer将会校验MRd TLP中的ECRC字段域，这个字段域是可选的不是必需的。若校验未出错，则将这个请求呈交给Device Core。然后根据这个请求命令中的相关信息，设备将会收集被请求的数据，并向Requester返回一个带有数据的完成包（Completion with Data TLP，CplD），同时它也将在自己的重传Buffer中保存这个CplD TLP的副本。当Switch接收到这个CplD TLP时将会对其进行错误校验。

**步骤3b****：**校验未发现出错，因此Switch向Completer返回一个Ack DLLP。作为响应，Completer将其重传Buffer中保存的这个CplD TLP的副本清除。

**4.**    **步骤4a****：**Switch对这个CplD TLP中的Requester ID进行译码，并通过这个信息将CplD TLP路由至正确的输出端口，输出的同时在这个输出端口的重传Buffer内保存CplD TLP的副本。当Requester接收到这个CplD TLP时将会对其进行错误校验。

**步骤4b****：**校验未发现出错，因此Requester向Switch返回一个Ack DLLP。作为响应，Switch之前输出这个CplD TLP的输出端口会将其重传Buffer中保存的这个CplD TLP的副本丢弃。Requester将校验ECRC字段域（这个字段是可选的，不是必需的），校验未出错则将完成包里的数据向上呈交给Device Core逻辑。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image100.jpg)

图 2‑27使用Ack/Nak协议的Non-Posted事务

##### 2.2.3.3   流量控制（Flow Control）

数据链路层的第二个主要功能就是流量控制。在系统上电和复位之后，流量控制机制被数据链路层的硬件自动初始化，并在整个运行过程中更新。关于这些内容的概述已经在TLP的那一节给出，所以这里不再赘述。想要了解更多关于这一主题的内容，请参阅Chapter 6“Flow Control”。

##### 2.2.3.4   电源管理（Power Management）

最后要介绍的一点，数据链路层也参与了电源管理，因为链路与系统电源状态相关的请求和握手也可以通过DLLP来进行。想了解关于这一主题的更详细的内容，请参阅Chapter 16“Power Management”。

#### 2.2.4     物理层（Physical Layer）

##### 2.2.4.1   整体说明（General）

物理层时PCIe协议层次里的最底层，如图 2‑14。TLP和DLLP这两种类型的数据包都需要从数据链路层向下转发至物理层，这样才能通过链路传输至对端接收方设备，并从接收方的物理层向上转发至它的数据链路层。协议规范中将关于物理层的讨论分为两部分：逻辑部分和电气部分。我们这里也将保留这种划分方式。逻辑物理层（Logical Physical Layer）包含了一系列的数字逻辑，这些数字逻辑是关于准备将数据包在链路上进行串行传输的逻辑以及相反的输入数据包的处理逻辑。电气物理层（Electrical Physical Layer）是物理层的模拟电路接口，它与链路直接相连，它为每个通道（Lane）提供差分驱动器（differential driver）以及差分接收器（differential receiver）。

##### 2.2.4.2   逻辑物理层（Physical Layer-Logical）

由数据链路层转发来的TLP与DLLP被物理层中的buffer所记录，在这个buffer中会对这些数据包加上包起始字符和包结束字符，这两个字符将有助于接收端用来检测数据包的边界。由于包起始字符和包结束字符分别出现在数据包的两端（头和尾），因此他们也被称作“组帧（framing）”字符。如图 2‑28展示了组帧字符被添加在TLP和DLLP上，它也展示了这两种字符的字段长度。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image102.jpg)

图 2‑28物理层中的TLP与DLLP的结构

在物理层中，数据包中的每一个字节都将被分割到链路所使用的所有通道，这一过程被称为字节条带化（Byte Striping）。实际上，每个通道在链路上都扮演着一个独立的串行传输路径，这些通道们各自传输的数据会在接收端被聚合。每个字节都进行了扰码，以此来减少在传输线上传输连续重复的“0”或“1”，这有助于减少链路上的EMI（electro-magnetic interference，电磁干扰）。对于前两代的PCIe（Gen1和Gen2），8bit的字符将被编码为10bit“符号（symbol）”，所使用的编码方式被称为8b/10b编码。这种编码增加了输出数据流的额外开销，但是也带来了不少有用的特性（关于这里的更多内容请见“8b/10b Encoding”一节）。PCIe Gen3的物理层逻辑在使用Gen3速率进行传输时（因为也可以向下兼容Gen1、Gen2速率），不再使用8b/10b编码方式，而是采用了另一种被称为128b/130b的编码，它会将数据包的字节扰码后进行传输。每个通道（Lane）的10b符号（对于Gen1 Gen2）或者每个通道的数据包的字节（对于Gen3）随后就会在链路的每个通道上以串行差分的形式被传输，对应的速率为Gen1 2.5GT/s，Gen2 5GT/s，Gen3 8GT/s。

当数据包的bit到达接收端时，接收端以训练完成的时钟速率对这些数据包的bit进行接收。如果使用的是8b/10b编码，那么输入的比特流将被串并转换器转换成10bit符号，然后准备用来做8b/10b解码。然而，在解码之前，10bit符号将会经过一个弹性缓存（elastic buffer），这是一个聪明的设备，它可以对两个相连接设备各自内部时钟之间的微小频率差异进行补偿。接下来，10bit符号被8b/10b解码器解码，正确地恢复成8bit字符。PCIe Gen3与前面所述的这个过程不同，对于Gen3的物理层逻辑来说，当接收到以Gen3速率传输的数据包串行比特流时，将使用串并转换器将这个比特流转换为字节流，这个串并转换器已经建立了块锁定（Block Lock，这是链路训练中的一个要素）。随后字节流会通过一个弹性缓存进行时钟容忍度补偿（clock tolerance compensation）。在Gen3速率下数据包数据并没有使用8b/10b编码，因此可以跳过8b/10b解码这一步。最终，每个通道中的8bit字符都将经过解扰，然后经过反条带化（un-striped）之后将所有通道内的字节聚合重新恢复成单符号流（single character stream），这样就在接收端恢复出了发送端所发出的数据流。

##### 2.2.4.3   链路训练和初始化（Link Training and Initialization）

物理层的另一个任务就是负责链路的初始化以及训练过程。在这个全自动化的过程中，为了让链路准备好进行正常工作，我们采用了几个步骤，主要为确定几个可选条件的状态。例如，链路宽度可以从1通道到32通道，并且可以提供多种速度。链路训练过程将会发现这些可选项并通过一个状态机来寻求一个建立最佳连接的组合，也就是说链路训练将会确认这些可选项具体的值（链路宽度、速率等）。在这个过程中，为了确保执行正确以及最优的操作，我们需要检查或者建立一些东西，例如：

u 链路宽度（Link width）

u 链路数据率（Link data rate）

u 通道调转（Lane reversal）——通道收发连接相反了，本应TX连对端RX，若相反则变成TX连了对端TX。

u 极性反转（Polarity inversion）——通道极性连接相反，TX_P应当连接对端的RX_P，若相反则变成了TX_P连接了对端的RX_N。

u 每个通道的位锁定（bit lock）——恢复出发送方的时钟。

u 每个通道的符号锁定（symbol lock）——在比特流中找到一个可辨认的位置。

u 多通道链路中的Lane-to-Lane歪斜去除（de-skew）。

##### 2.2.4.4   电气物理层（Physical Layer-Electrical）

物理的发送器和接收器是通过一个AC耦合链路相连接，如图 2‑29所示。所谓“AC耦合（交流耦合）”，意思是两个设备相连接的物理路径上放置有电容，它用于让信号的高频部分（AC交流）通过，而阻塞低频（DC直流）部分。许多串行传输方式中都使用了这种方法，因为它允许发送端和接收端的共模电压不同（共模电压指的是0、1电平交界处的电压），这意味着发送端和接收端可以使用不同的参考电压。当两个设备的物理连接距离很近时，这一特性优势并不明显，而若它们距离较远例如在不同的楼里，那么它们将很难使用一个完全相同的公共的参考电压。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image104.jpg)

图 2‑29 电气物理层

##### 2.2.4.5   命令集（Ordered Sets）

最后要介绍的一种在设备之间发送的流只使用物理层。尽管这种信息对于接收端来说很容易进行识别，但是它并没有被做成数据包的形式，原因是比如它没有包起始字符和包结束字符。因此作为一种替代方法，这种信息被组织成了一种被叫做“命令集（Ordered Sets）”的东西，它起始于发送端的物理层，终止于接收端的物理层，如图 2‑30所示那样。对于Gen1和Gen2的数据率下，一个命令集使用一个单独的COM字符作为起始，然后后面接着3个或以上的其他字符用于定义要发送的信息。关于这些不同类型字符在PCIe中的命名的更细节的讨论请见“Character Notation”一节，对于现阶段来说只要知道COM字符的特性能让我们达到想要的设计目标即可。命令集的大小总是4byte的整数倍，图 2‑31展示了一个命令集的例子。在Gen3操作模式中，命令集的格式就不同于上述的Gen1/Gen2格式了。更多详细内容请见Chapter 14“Link Initialization & Training”。命令集永远只会终止于链路的对端设备，而不会被Switch路由转发至PCIe网络中，也就是说对端设备如果是Switch那么它的最终目的地就是Switch。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image106.jpg)

图 2‑30命令集的出发地和目的地

命令集在链路训练（Link Training）过程中同样有应用，请参阅Chapter 14“Link Initialization & Training”。它们也被用于补偿发送端和接收端内部时钟的轻微差异，这一过程被称为时钟容忍度补偿（clock tolerance compensation）。最后一点，命令集还可以用于指示链路进入或者退出低功耗状态。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image108.jpg)

图 2‑31命令集结构

### 2.3 协议回顾（Protocol Review Example）

现在，让我们通过一个示例来回顾整个链路协议，这个例子将从Requester发起一个Memory Read请求（MRd）开始，直到Requester从Completer获得所请求的数据为止，讲解这过程中所发生的操作步骤。

#### 2.3.1     Memory Read Request（MRd请求）

关于我们开始讨论的第一部分，请参考图 2‑32。Requester的Device Core或者说是软件层将会向事务层发起一个请求，这个请求中包含了这些信息：32bit或64bit的memory地址、事务类型、需要读取的数量总量（以dw为单位计数）、流量类型、字节使能、属性等等。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image110.jpg)

图 2‑32 Memory Read请求的各阶段

事务层使用上面所述的那些信息来构建一个MRd TLP。关于TLP内部格式的细节我们稍后再进行描述，目前我们只需要知道TLP的Header大小为3DW或4DW，这取决于它的地址字段的大小（32bit地址对应3DW Header，64bit地址对应4DW Header）。此外，在Header中还有事务层加入的Requester ID字段（bus#，device#，function#），Completer可以通过这个Requester ID字段来返回完成包。TLP随后被置入相应优先级的虚拟通道buffer中，等待轮到它被发送。一旦这个TLP被选中，流量控制逻辑将会确认对端设备的接收buffer（VC，虚拟通道）有足够的可用空间来接收这个TLP，然后MRd TLP就被向下发送至数据链路层。

在数据链路层中，数据包被加上12bit的序列号（Sequence Number）以及32bit的LCRC。并在重传Buffer（Replay Buffer）中保存这个TLP的一个副本，然后这个数据包就被向下转发至物理层。

在物理层中，数据包被加上包起始字符以及包结束字符，然后在所有可用的通道上进行字节条带化（byte striping），再进行扰码（Scramble）、8b/10b编码。最终这个数据包的比特在链路上的每个通道内以串行差分的形式传输到对端。

Completer接收到输入的比特流，它将这个比特流先进行串并转换将比特流恢复成10bit符号，然后让它们通过一个弹性缓存。随后10bit符号被解码后恢复成字节，然后每个通道上的字节都被解扰（de-scramble）以及反条带化（un-striped）。接下来Completer物理层检测到包起始字符和包结束字符，并将它们从TLP中剥除。剩余的TLP被向上转发至数据链路层。

Completer的数据链路层将对接收到的TLP进行LCRC错误校验，并检查TLP序列号以确定是否存在TLP丢失或TLP失序。若这些步骤都无错，数据链路层将生成一个Ack信息，里面包含了与这个MRd TLP中相同的序列号。然后再给这个Ack信息加上16bit的CRC，这样就组成了一个Ack DLLP，将这个Ack DLLP送回物理层，由物理层加上相应的组帧符号（framing symbol），并把Ack DLLP传输给Requester。

Requester的物理层接收到Ack DLLP后，对其组帧符号进行检查和剥除，然后向上转发至数据链路层。如果数据链路层对其CRC校验无错，它将会用Ack DLLP内的序列号与重传Buffer中保存的TLP副本的序列号进行比较，并将与Ack DLLP中序列号相匹配的TLP副本从重传Buffer中清除。相反地，若Requester接收到的是一个Nak DLLP，那么它将把序列号匹配的这个MRd TLP进行重传。由于DLLP仅对数据链路层有意义，所以在这些DLLP的操作中将不会有东西向上转发给事务层。

除了上述的产生Ack DLLP之外，Completer的数据链路层还将把TLP向上转发给事务层。在Completer的事务层中，TLP被置于与其优先级相对应的虚拟通道接收buffer中，等待被处理。然后可以对TLP进行ECRC校验（ECRC为可选项），若校验无错，那么TLP Header的内容（地址、Requester ID、MRd事务类型、请求数据总量、流量类型等）将被向上转发至Completer的软件层。

#### 2.3.2     Completion with Data（CplD带有数据的完成包）

下面开始介绍此次举例回顾PCIe协议的另一半内容，请参考图 2‑33。为了服务MRd请求，Completer的Device Core/软件层将会向它的事务层发送一个带有数据的完成包（Completion with Data，CplD）请求，这个完成包请求中包含了与MRd请求中相同的Requester ID、Tag，还包含了事务类型以及另外的完成包头内容，并且完成包中还包含了被请求的数据。

![img](file:///C:/Users/FANLI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image112.jpg)

图 2‑33带有数据的完成包的各阶段

事务层使用CplD请求中的这些信息来构建一个CplD TLP，这种TLP的Header固定为3DW（这是由于CplD TLP使用Requester ID作为路由信息，因此不会需要用到64bit的地址）。事务层也会将自身的Completer ID添加到CplD TLP Header中。这个数据包随后被放置入一个合适的虚拟通道的发送Buffer中，一旦这个虚拟通道被仲裁选中并要发送这个CplD TLP，流量控制逻辑将会确认对端设备有足够的可用缓冲区空间来接收它，当确认足够后则将这个CplD TLP向下转发至数据链路层。

如前文所述那样，数据链路层给数据包加上12bit的序列号（Sequence Number）和32bit的LCRC。然后将添加完这些信息的TLP保存一份副本在重传Buffer（Replay Buffer）中，之后变将数据包向下转发至物理层。

依然如前文所述那样，物理层给数据包加上包起始字符和包结束字符，并将其在所有可用通道上进行字节条带化，再进行扰码、8b/10b编码。最终，这个CplD TLP数据包在链路的所有通道上以串行差分的形式被传输至对端。

当Requester接收到这个CplD TLP的输入比特流时，它将比特流转换恢复成10bit符号（10bit symbol），并让10bit symbol流通过一个弹性缓存。随后10bit符号被解码恢复为字节，并进行解扰和字节反条带化（un-striped）。然后物理层就能检测到这个CplD TLP的包起始字符和包结束字符，并将这两个字符剥去。之后便将剩余的CplD TLP向上转发至数据链路层。

和之前一样，数据链路层对CplD TLP进行LCRC校验，并检查序列号以确定是否存在TLP丢失或出现TLP失序。如果并未出现错误，数据链路层将产生一个Ack DLLP，其中包含了与CplD TLP中相同的序列号，并给这个Ack DLLP加上16bit CRC，然后将其送回给物理层加上相应的组帧字符（framing symbol）并将这个Ack DLLP传输给Completer。

Completer的物理层将会检测并剥除Ack DLLP的组帧字符，并将剩余的部分向上转发给数据链路层，在这里对Ack DLLP的CRC进行校验。若校验无错，Completer的数据链路层将Ack DLLP中的序列号与重传Buffer中的TLP的序列号进行对比。与Ack DLLP序列号相匹配的TLP将被从重传Buffer中清除。反之，若Completer接收到的是一个Nak DLLP，那么它将使用重传Buffer中存储的CplD TLP副本来进行重传。

与此同时，Requester的事务层在某个虚拟通道的缓存中接收到了这个CplD TLP。作为一个可选操作，事务层可以校验这个TLP的ECRC。若校验无错，事务层将这个CplD TLP的Header内容、数据荷载以及请求完成状态信息向上呈交给Requester的软件层。至此，大功告成。

 

